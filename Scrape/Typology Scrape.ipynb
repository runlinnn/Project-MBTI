{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2 \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "    'referrer': 'https://google.com',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Pragma': 'no-cache'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thread_links(url):\n",
    "    '''Retrieves all the thread links in ONE page of the forum'''\n",
    "    links = []\n",
    "    scrapping = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(scrapping.text, 'html.parser')\n",
    "    html_links = soup.findAll('a',attrs={'class': 'title'})\n",
    "    links = [x.get('href') for x in html_links]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_thread_links(forum_link, start_page, end_page):\n",
    "    '''Extracts all thread links from the A NUMBER of pages of the forum'''\n",
    "    scrapped_links = []\n",
    "    url = forum_link\n",
    "\n",
    "    for i in range(start_page, end_page):\n",
    "        url += 'index'+ str(i)+ '.html'\n",
    "        thread_links = get_thread_links(url)\n",
    "        scrapped_links.extend(thread_links)\n",
    "        url = forum_link\n",
    "        \n",
    "        \n",
    "    return scrapped_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mbti(string):\n",
    "    \n",
    "    '''Retrieves MBTI tag from each post. Posts that do \n",
    "    not have MBTI tag will return 0'''\n",
    "    \n",
    "    index = string.find('MBTI')\n",
    "    \n",
    "    if index == -1:\n",
    "        return 0\n",
    "    else:\n",
    "        mbti = string[index+5:index+5+4]\n",
    "        return mbti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_page_comments(link):\n",
    "    page_comments = []\n",
    "    page_mbti = []\n",
    "    \n",
    "    request = requests.get(link,headers=headers)\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        \n",
    "    block_comment = soup.find_all('div', attrs={'class': 'postdetails'})\n",
    "    \n",
    "    for block in block_comment:\n",
    "        user_info = block.find('dl', attrs={'class': 'userinfo_extra'})\n",
    "        comment = block.find('blockquote', attrs={'class': 'postcontent restore'})\n",
    "\n",
    "        if comment == None: #extracted some other parts of the webpage that is not the posts\n",
    "            continue\n",
    "\n",
    "        # retrieve mbti tag\n",
    "        if user_info != None:\n",
    "            user_info_text = user_info.get_text()\n",
    "            mbti = get_mbti(user_info_text)\n",
    "        else:\n",
    "            mbti = 0\n",
    "            \n",
    "        page_mbti.append(mbti)\n",
    "\n",
    "        # retrieved the comment from post with reply box\n",
    "        bbcode = block.find('div', attrs={'class': 'bbcode_container'})\n",
    "        if bbcode != None:\n",
    "            acc_comments = ''\n",
    "            while bbcode != None:\n",
    "                bbcode = bbcode.next_sibling\n",
    "                if str(bbcode) == '<br/>' or bbcode == None:\n",
    "                    continue\n",
    "                elif bbcode.name != None:\n",
    "                    continue\n",
    "                else:\n",
    "                    acc_comments += bbcode.strip()\n",
    "                    \n",
    "            page_comments.append(acc_comments)\n",
    "\n",
    "        else: # comment does not come with reply box\n",
    "            comment = comment.get_text().strip()\n",
    "            page_comments.append(comment)\n",
    "    \n",
    "    return page_mbti, page_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subpages(link): \n",
    "    ''' Finds out what is the last page of one thread link\n",
    "    and the index to increase the page number'''\n",
    "    request = requests.get(link,headers=headers)\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    \n",
    "    page_navigator = soup.find_all('div', attrs={'class': 'paginationList'})\n",
    "    \n",
    "    for tag in page_navigator:\n",
    "        link_tag = tag.find_all(\"a\")\n",
    "\n",
    "        if len(link_tag) != 0:\n",
    "            last_link = link_tag[-1].get('href')\n",
    "            html_index = last_link.find('.html')\n",
    "            last_hyphen_index = last_link.find('-',html_index - 4,html_index)\n",
    "            last_page = last_link[last_hyphen_index+1:html_index]\n",
    "            \n",
    "            #check if last page retrieved is only a number. If not retrieve number again\n",
    "            inc = 1\n",
    "            while last_page.count('-') != 0:\n",
    "                last_hyphen_index = last_link.find('-',html_index + inc - 4,html_index)\n",
    "                last_page = last_link[last_hyphen_index+1:html_index]\n",
    "                inc += 1\n",
    "\n",
    "            return int(last_page)\n",
    "        \n",
    "        else:\n",
    "            html_index = link.find('.html')\n",
    "            return 1\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_mbti_comments(all_thread_links):\n",
    "    personality = []\n",
    "    post = []\n",
    "    for i in range(len(all_thread_links)):\n",
    "        link = all_thread_links[i]\n",
    "        request = requests.get(link,headers=headers)\n",
    "        soup = BeautifulSoup(request.text, 'html.parser')\n",
    "        \n",
    "        # within thread link, get the number of pages and the index to change the link\n",
    "        num_subpages = get_subpages(link)\n",
    "        html_index = link.find('.html')\n",
    "        \n",
    "        # get comments from all the subpages\n",
    "        for i in range(num_subpages):\n",
    "            subpage_link = link[:html_index] + '-' + str(i) + link[html_index:]\n",
    "            mbti, comments = get_one_page_comments(subpage_link)\n",
    "            \n",
    "            personality.extend(mbti)\n",
    "            post.extend(comments)\n",
    "        \n",
    "    return personality, post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_link = 'https://www.typologycentral.com/forums/forum22/'\n",
    "scrapped_links = get_all_thread_links(forum_link, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti, comment = get_all_mbti_comments(scrapped_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mbti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"MBTI\": mbti, \"Post\": comment}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Forum23_1_20.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
